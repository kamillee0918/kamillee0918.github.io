---
layout: post
title: "SPSA: 게임 AI를 위한 파라미터 자동 조정 기법"
date:   2025-08-12 10:00:00 +0900
last_modified_at: 2025-08-12 10:00:00 +0900
tags: [SPSA, ParameterTuning, YaneuraOu, Stockfish, MachineLearning, 일본장기, 자동튜닝]
toc: true
---

## 📑 **Table Of Contents**

- [1. ⚙ 파라미터 튜닝의 역사와 한계](#history-and-limits)
  - [수동 튜닝에서 베이즈 최적화까지](#from-manual-to-bayes)
  - [쇼기 AI와 베이즈 최적화의 상성](#shogi-ai-and-bayes)
- [2. ⚙ SPSA 알고리즘: 단순함의 미학](#spsa-algorithm)
  - [핵심 아이디어](#core-idea)
  - [알고리즘 상세 설명](#algorithm-details)
- [3. ⚙ 실전 팁과 성공 사례](#practical-tips)
- [4. 🏁 마치며](#conclusion)

---

![spsa.png](/images/posts/2025-08-12-spsa-search-params/spsa.png)

## 1. ⚙ 파라미터 튜닝의 역사와 한계 {#history-and-limits}

### 수동 튜닝에서 베이즈 최적화까지 {#from-manual-to-bayes}

게임 AI의 성능을 끌어올리기 위한 탐색 파라미터 조정은 오랜 숙제였습니다. 2015년경, Ponanza와 같은 최고 수준의 쇼기 AI조차 각 파라미터를 하나씩 수동으로 변경하고 수많은 자체 대국을 통해 승률을 확인하는 고된 작업을 반복했습니다.

당시 야네우라오(やねうら王)는 조금 다른 접근법을 시도했습니다. 모든 파라미터를 동시에 무작위로 조금씩 변경하고, 그 결과를 통계적으로 집계하여 승률이 개선되는 방향으로 파라미터를 일괄 조정하는, 마치 '확률적 경사 하강법(SGD)'과 유사한 방식이었습니다.

이후 `Hyperopt`나 `Optuna`와 같은 베이즈 최적화(Bayesian Optimization) 라이브러리가 등장하며 파라미터 튜닝 자동화에 새로운 길이 열렸습니다. dlshogi와 같은 일부 쇼기 AI 프로젝트에서 이를 활용한 자체 튜닝 스크립트를 개발하기도 했습니다.

### 쇼기 AI와 베이즈 최적화의 상성 {#shogi-ai-and-bayes}

하지만 베이즈 최적화 기법은 쇼기 AI, 특히 NNUE 계열 엔진의 파라미터 튜닝에 널리 사용되지는 못했습니다. 그 이유로 추정되는 것은 쇼기 AI 파라미터의 특성 때문입니다.

- **고차원성**: 조정해야 할 파라미터의 수가 매우 많습니다.
- **비연속성 및 조건부 의존성**: 특정 조건에서만 활성화되거나, 다른 파라미터와 복잡하게 얽혀있는 경우가 많습니다.

이러한 특성은 정교한 확률 모델을 가정하는 베이즈 최적화가 효율적으로 작동하기 어려운 환경을 만듭니다.

---

## 2. ⚙ SPSA 알고리즘: 단순함의 미학 {#spsa-algorithm}

이러한 상황에서 체스 AI 세계를 중심으로 대안으로 떠오른 것이 바로 **SPSA(Simultaneous Perturbation Stochastic Approximation)** 입니다. Stockfish의 테스트 프레임워크인 `fishtest`나 `OpenBench`에 SPSA가 도입되면서, 체스 AI 개발자 대부분이 이를 표준적인 튜닝 방식으로 채택하게 되었습니다.

### 핵심 아이디어 {#core-idea}

SPSA의 아이디어는 놀라울 정도로 단순합니다. 모든 파라미터를 한 번에, 하지만 각기 다른 랜덤 방향으로 조금씩 흔들어 본 뒤, 더 좋은 결과를 보인 방향으로 전체 파라미터를 미세하게 이동시키는 과정을 반복하는 것입니다.

### 알고리즘 상세 설명 {#algorithm-details}

SPSA 알고리즘은 단 1분이면 설명이 끝날 정도로 간단합니다.

1.  **현재 파라미터 벡터 `θ`** 에서 시작합니다.
2.  각 요소가 +1 또는 -1을 1/2 확률로 갖는 **랜덤 방향 벡터 `Δ`** 를 생성합니다. (이를 '라데마커 분포'라고 합니다.)
3.  파라미터별 변화 강도를 나타내는 **척도(scale) 벡터 `s`** 를 `Δ`와 곱하여(`s⊙Δ`) 실제 변화량을 계산합니다.
4.  두 가지 버전의 파라미터, 즉 **`θ + s⊙Δ`** 와 **`θ - s⊙Δ`** 를 각각 적용한 AI로 대국을 진행합니다.
5.  만약 `+s⊙Δ` 방향이 승리했다면, 그 방향으로 현재 파라미터 `θ`를 업데이트합니다. 업데이트 양은 `ε * s⊙Δ / 2` 와 같이 아주 작은 값(`ε`는 학습률)을 사용합니다.

여기서 분모에 2가 들어가는 이유는, 이 과정이 다변수 함수에서 기울기를 근사하는 '중심 차분 근사'와 수학적으로 유사하기 때문입니다.

---

## 3. ⚙ 실전 팁과 성공 사례 {#practical-tips}

- **실수(float) 연산**: 소스코드 상에서는 정수(int)인 파라미터라도, SPSA 프레임워크 내부에서는 부동소수점(float)으로 관리하는 것이 중요합니다. 이를 통해 1 미만의 미세한 변화를 누적시켜 최적점에 점진적으로 도달할 수 있습니다.
- **어닐링(Annealing)**: 튜닝 초기에는 변화의 폭(`s`)과 학습률(`ε`)을 크게 설정하고, 튜닝이 진행됨에 따라 점차 줄여나가는 것이 효과적입니다.

야네우라오 프로젝트에서는 이 SPSA 알고리즘을 기반으로 자체 튜닝 프레임워크를 개발했으며, 이를 통해 **레이팅(Rating)을 약 +50점** 가량 끌어올리는 성과를 거두었습니다.

---

## 4. 🏁 마치며 {#conclusion}

SPSA는 누구나 떠올릴 법한 매우 단순한 알고리즘이지만, 쇼기 AI와 같이 복잡하고 고차원적인 파라미터 공간에서는 정교한 베이즈 최적화보다 오히려 더 나은 성능을 보여준다는 점에서 놀라움을 줍니다.

구현이 간단하고 다양한 게임 AI의 파라미터 튜닝에 곧바로 적용할 수 있다는 점에서, SPSA는 앞으로도 널리 활용될 잠재력을 가진 강력한 기법이라고 할 수 있습니다.
