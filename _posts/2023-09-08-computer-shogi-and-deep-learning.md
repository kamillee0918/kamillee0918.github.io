---
layout: post
title: 컴퓨터 쇼기에 의한 딥러닝 고찰
date: 2023-09-08 16:22 +0800
last_modified_at: 2023-09-08 16:22 +0800
tags: [shogi, deep-learning]
toc: true
---

[지난 포스팅]({% link _posts/2023-09-08-bonanza-method.md %})에서 컴퓨터 쇼기와 알파고에 대하여 언급했었는데, Ponanza가 쇼기로 딥러닝을 어떻게 해서 적용되었는지를 알아보겠습니다.

전왕전 인터뷰에서 프로 기보와의 일치율을 언급한 점을 고려하면, 학습 대상은 평가 함수가 아니라 정책(Policy)인 것으로 판단됩니다.

이는 형세(盤面) 정보를 입력으로 받아, 착수(指し手) 확률을 출력하는 함수입니다.

### 입력(Input)

핵심은 입력되는 형세의 정보 제공 방식이라고 생각합니다.

알파고는 19x19 크기 보드 상 각 좌표에 해당하는 검은 돌, 흰 돌, 빈 칸 이렇게 세 가지 상태를 나타내는 총 세 개 채널과 추가적인 바둑 지식 (예: 단수[アタリ], 축[シチョウ] 등) 을 각각 다른 채널로 처리하여 모든 데이터를 합쳤을 때 최대 48개까지 입력할 수 있었습니다.

반면 쇼기는 기물의 종류가 다양하므로 승격(promote; 成り)을 포함하면 14가지의 기물 수만큼의 채널이 필요합니다.

또한 수 중의 기물(持ち駒)도 포함되어야 하므로 9x9 좌표 외에도 별도로 표현될 필요가 있습니다.

컨볼루션에서는 위치 정보가 중요하기 때문에 어떤 식으로든 위치에 대해 노이즈가 발생하지 않도록 입력해 줄 필요가 있습니다.

또한 알파고에서는 쇼기 형세 정보 외에도 단수나 축의 위치 등을 제공하기 때문에, 쇼기의 지식으로 2보(二歩, 반칙)의 위치 정보 등은 제공해 주는 것이 좋을 것 같습니다.

### 출력(Output)

출력은 알파고에서는 돌을 놓는 위치를 나타내는 19x19의 1채널이지만 쇼기의 경우에는 좌표 외에도 어떤 기물이 움직일지, 어떤 기물이 승격할지, 수 중의 기물로 이용할 것인지 등의 정보가 필요로 합니다.

인터뷰에서는 규칙을 알려주지 않아도 정확한 결과가 도출되었다고 언급했기 때문에, 이동에 대해서는 고려하지 않고 좌표와 기물만을 출력해서 어디서 기물이 움직였는지는 별도의 로직으로 찾아내는 것 같습니다.

그러나 이 방식은 어디서부터 어느 위치로 기물이 움직였는지 파악하기 어렵다는 문제점이 있습니다.

이동 전과 이동할 곳의 좌표를 각각의 채널로 출력하고 있는 것일지도 모릅니다.

### 네트워크 구성

신경망의 구조에 대해 말하자면, 알파고는 풀링이 없는 13층 컨볼루션 신경망을 사용했습니다. 쇼기에서도 이와 유사한 방식으로 신경망을 구성할 수 있습니다.

### 학습

앞서 입력, 출력, 네트워크 구성이 설계되었다면, 알파고와 마찬가지로 프로의 기보를 교사 데이터로 삼아 오차 역전파로 학습할 수 있게 됩니다.

알파고에서 지도학습을 위한 policy network 학습에는 약 50개 가량의 GPU를 사용하여 약 3주가 소요되었습니다. 이를 통해 상당한 계산 자원이 필요함을 알 수 있습니다.

### 정책(Policy) 개선하기

알파고의 강화학습 policy network와 유사한 방식으로 알파고 간의 대국을 통한 강화학습으로 정책을 개선할 수 있습니다.

알파고의 논문에는 아래의 논문이 인용되었습니다.

> [Williams, R. J. Simple statistical gradient-following algorithm for connectionist reinforcement learning. Mach. Learn. 8, 229–256 (1992).](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf?pdf=button)

해당 논문에 제안된 사항으로는 REINFORCE 알고리즘이라는 함수 근사화 기법을 사용하여 강화학습을 일반화하는 것입니다.

### 평가함수 학습

알파고의 value network와 유사한 방식으로 정책에서 가치 함수를 학습할 수 있습니다.

다양한 게임 상황들에서 강화학습된 policy network가 제시하는 전략대로 게임을 끝까지 진행하면서 얻은 보상 결과를 바탕으로 회귀 분석을 활용하여 가치 함수를 학습합니다.

### 대국 시의 방법

알파고에서는 몬테카를로 나무 탐색의 착수 예측에 지도학습 policy network를 사용하고, 플레이 아웃의 결과와 value network의 결과를 평균화하여 기존 방식과 잘 결합하고 있습니다.

딥러닝에서는 읽기 부분에서 약점이 있으므로 플레이 아웃으로 보완하고 있는 것으로 보입니다.

쇼기는 바둑보다 판독이 중요한 게임이기 때문에 딥러닝만으로는 강하지 않을 것 같으며, 탐색과 잘 결합해야 비로소 완성될 것 같습니다.

[지난 포스팅]({% link _posts/2023-09-08-bonanza-method.md %})에서 언급했듯이, 기존의 탐색 방법을 활용하고, 학습된 정책을 가지치기에 사용하며, 학습된 가치 함수를 탐색의 말단 평가 함수에 적용함으로써 딥러닝과 기존의 탐색 방법을 효과적으로 결합할 수 있을 것으로 예상합니다.