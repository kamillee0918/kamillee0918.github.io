---
layout: post
title: 컴퓨터 쇼기에 의한 딥러닝 고찰
date: 2023-09-08 16:22 +0800
last_modified_at: 2023-09-08 16:22 +0800
tags: [Shogi, Deep Learning]
toc: true
---

[지난 포스팅](https://kamillee0918.github.io/2023/09/08/bonanza-method/)에서 컴퓨터 쇼기와 알파고에 대하여 언급했었는데, Ponanza가 쇼기로 딥러닝을 어떻게 해서 적용되었는지를 알아보겠습니다.

전왕전 인터뷰에서도 프로 기보와의 일치율에 대해 언급했던 것으로 보아 학습하는 것은 평가함수가 아닌 정책(Policy)인 것으로 생각됩니다.

입력으로 형세(盤面) 정보를 주고, 출력으로 착수(指し手) 확률을 출력하는 함수입니다.

### 입력(Input)

핵심 포인트는 입력되는 형세의 정보 제공 방식이라고 생각합니다.

알파고에서는 19x19의 각 좌표의 흑돌, 백돌, 빈칸 3개 채널(입력값: 2) 정보와 단수(アタリ)나 축(シチョウ) 등 약간의 바둑 지식을 특징량 형태로 각 채널에 할당하여 총 48개 채널의 데이터를 입력으로 삼았습니다.

반면 쇼기는 기물의 종류가 다양하기 때문에 승격(promote; 成り)을 포함하면 14가지의 기물 수만큼의 채널이 필요합니다.

또한 수 중의 기물(持ち駒)도 포함되어야 하기 때문에 9x9 좌표 외에도 별도로 표현될 필요가 있습니다.

컨볼루션에서는 위치 정보가 중요하기 때문에 어떤 식으로든 위치에 대해 노이즈가 발생하지 않도록 입력해 줄 필요가 있습니다.

또한 알파고에서는 쇼기 형세 정보 외에도 단수나 축의 위치 등을 제공하기 때문에, 쇼기의 지식으로 2보(二歩, 반칙)의 위치 정보 등은 제공해 주는 것이 좋을 것 같습니다.

### 출력(Output)

출력은 알파고에서는 돌을 놓는 위치를 나타내는 19x19의 1채널이지만 쇼기의 경우에는 좌표 외에도 어떤 기물이 움직일지, 어떤 기물이 승격할 지, 수 중의 기물로 이용할 것인지 등의 정보가 필요로 합니다.

인터뷰에서는 규칙을 알려주지 않아도 정확한 결과가 도출되었다고 언급했기 때문에, 이동에 대해서는 고려하지 않고 좌표와 기물만을 출력해서 어디서 기물이 움직였는지는 별도의 로직으로 찾아내는 것 같습니다.

이러한 경우에는 어디에서 어떤 기물이 이동하였는지를 알 수 없는 경우가 간혹 발생한다는 문제가 있습니다.

이동 전과 이동할 곳의 좌표를 각각의 채널로 출력하고 있는 것일지도 모릅니다.

### 네트워크 구성

신경망 구성은 알파고에서는 풀링이 없는 13층 컨볼루션 신경망이었지만 쇼기도 위와 비슷하게 구성해도 좋을 것 같습니다.

### 학습

앞서 입력, 출력, 네트워크 구성이 설계되었다면, 알파고와 마찬가지로 프로의 기보를 교사 데이터로 삼아 오차 역전파로 학습할 수 있게 됩니다.

알파고의 지도학습 policy network에서는 50여 개의 GPU로 3주 정도 소요되었기 때문에 상당량의 계산 리소스가 필요로 하는 것 같습니다.

### 정책(Policy) 개선하기

알파고의 강화학습 policy network와 유사한 방식으로 알파고 간의 대국을 통한 강화학습으로 정책을 개선할 수 있습니다.

알파고의 논문에는 아래의 논문이 인용되었습니다.

> [Williams, R. J. Simple statistical gradient-following algorithm for connectionist reinforcement learning. Mach. Learn. 8, 229–256 (1992).](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf?pdf=button)

해당 논문에 제안된 사항으로는 REINFORCE 알고리즘이라는 함수 근사화 기법을 사용하여 강화학습을 일반화하는 것입니다.

### 평가함수 학습

알파고의 value network와 유사한 방식으로 정책에서 가치 함수를 학습할 수 있습니다.

대량의 서로 다른 국면에서 강화학습 policy network의 방침에 따라 끝까지 플레이했을 때의 보상으로부터 회귀를 통해 가치 함수를 학습한다.

### 대국 시의 방법

알파고에서는 몬테카를로 나무 탐색의 착수 예측에 지도학습 policy network를 사용하고, 플레이아웃의 결과와 value network의 결과를 평균화하여 기존 방식과 잘 결합하고 있습니다.

딥러닝에서는 읽기 부분에서 약점이 있기 때문에 플레이아웃으로 보완하고 있는 것으로 보입니다.

쇼기는 바둑보다 판독이 중요한 게임이기 때문에 딥러닝만으로는 강하지 않을 것 같으며, 탐색과 잘 결합해야 비로소 완성이 될 것 같습니다.

[지난 포스팅](https://kamillee0918.github.io/2023/09/08/bonanza-method/)에 썼듯이 기존의 탐색을 사용하고, 학습한 정책을 가지치기에 사용하고, 학습한 가치 함수를 탐색의 말단 평가 함수에 사용함으로써 딥러닝을 기존의 탐색과 함께 사용할 수 있을 것으로 보입니다.
