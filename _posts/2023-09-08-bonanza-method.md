---
layout: post
title: Bonanza 메서드 해설
date: 2023-09-08 12:22 +0800
last_modified_at: 2023-09-08 12:35 +0800
tags: [shogi, deep-learning]
math: true
toc: true
---

컴퓨터 쇼기(일본 장기)는 예전부터 관심이 많았었고, 지난 2017년에 개최되었던 제4회 전왕전(電王戦)을 되짚어보려 합니다.

당시 명인(名人) 타이틀을 보유한 사토 아마히코(佐藤天彦)는 컴퓨터 쇼기 소프트인 [Ponanza(ポナンザ)](https://ja.wikipedia.org/wiki/Ponanza)에게 패하였고, 대국에서 사용된 버전은 탐색은 미니맥스 알고리즘(Minimax Algorithm)의 개선판으로, 평가 함수를 기계학습으로 최적화하는 방식으로 만들어져 있습니다.

미니맥스 알고리즘의 병렬화, 고속화는 체스 프로그램 등에서 오래 전부터 개선되어 왔습니다.

쇼기 프로그램에서도 기본은 크게 다르지 않으며 컴퓨터 쇼기에서의 기술적인 발전은 그다지 크지 않았다고 생각합니다.

평가 함수에 관해서는 2006년 [Bonanza(ボナンザ)](https://ja.wikipedia.org/wiki/Bonanza) 등장 이전에는 개발자가 로직을 짜서 파라미터 등을 조정하는 것이 보통이었지만 Bonanza는 프로 기사들의 기보로부터 KPP(왕(King) 기물을 중심으로 특정 말의 기물들이 어떠한 형태(囲い)를 맺고 있는지)를 통해 이러한 특징값의 선형 합을 기계학습하면서 컴퓨터 쇼기의 획기적인 발전을 이루었습니다.

이 소프트의 등장 이후 대부분의 쇼기 프로그램은 Bonanza에서 비롯되었다고 할 수 있습니다.

Ponanza는 다른 쇼기 프로그램과의 차별점은 평가함수 학습에 프로 기사들의 기보를 사용하지 않고, 자기 대국 결과를 이용해 강화학습을 하고 있는 것으로 추측됩니다.

Bonanza가 도입한 평가함수의 기계학습은 현재 쇼기 프로그램의 중요한 요소로 자리 잡고 있습니다.

이번 포스팅에서는 Bonanza가 진행하는 기계학습 방법(통칭 Bonanza 메서드)에 관하여 설명하겠습니다.

### Bonanza 메서드

학습할 국면 집합을 $$\mathbf{P}$$ 로 정의하고, 특징 벡터를 $$\mathbf{v}$$, 국면 p의 합법적인 수의 수를 $$M_p$$, 기보에서 선택된 자식 국면을 $$p_1$$ m번째 합법적인 수에 의한 자식 국면을 $$p_m$$라고 하면 목적 함수 $$J(\mathbf{P},\mathbf{v})$$를 다음과 같이 설계할 수 있습니다.

$$
J(\mathbf{P},\mathbf{v})=\sum_{p∈\mathbf{P}}\sum_{m=2}^{Mp} T_p[\xi(p_m, \mathbf{v})−\xi(p_1,\mathbf{v})]
$$

위의 수식에서 $$\xi(p_m, \mathbf{v})$$는 미니맥스 탐색치, $$T_p(x)$$는 손실 함수이며, 국면 p의 차례가 Max 플레이어인 경우 $$T_p(x)=T(+x)$$이고, Min 플레이어의 경우 $$T_p(x)=T(-x)$$가 됩니다.

$$T(x)$$는 단조로운 증가 함수인 경우가 좋으며 Bonanza에서는 시그모이드 함수 $$1/(1+e^−0.0273x)$$가 사용되고 있습니다.

경사하강법을 사용해 목표 함수를 최소화하도록 특징 벡터 $$\mathbf{v}$$를 학습합니다.

### 해설

프로의 기보에서 선택된 수에 대한 정확한 평가값을 알 수 없기 때문에 직접 평가함수를 학습할 수 없습니다.

따라서 현재의 평가함수를 사용하여 프로의 기보에서 선택한 국면의 평가값과 그 외의 국면의 평가값의 차이의 합을 손실함수 $$T(x)$$로 정의하고 있습니다.

프로의 기보와 다른 수를 선택했을 경우, 그 평가값을 낮게 추정할수록 손실함수의 값은 커집니다.

모든 국면분에 대해 손실함수를 계산한 합이 목적함수가 됩니다.

손실 함수를 미분 가능한 함수로 만들면, 그래디언트 방법을 이용해 특징 벡터 $$\mathbf{v}$$를 최적화할 수 있습니다.

경사하강법은 목적함수를 $$v_i$$ 에 대해 편미분을 수행합니다.

$$
\frac{\partial J(\mathbf{P},\mathbf{v})}{\partial v_i}=\sum_{p∈\mathbf{P}}\sum_{m=2}^{M_p} \frac{dT_p(x)}{dx} [\frac{\partial \xi(pm,\mathbf{v})}{\partial v_i} − \frac{\partial \xi(p1,\mathbf{v})}{\partial v_i}]
$$

각 파라미터 $$v_i$$에 대해 편미분 값에 학습률을 곱하여 파라미터를 업데이트합니다.

위 식에서 $$T_p(x)$$의 $$v_i$$에 대한 편미분은 합성함수의 미분법칙

$$
y=f(x), x=g(\mathbf{v})일 때, \frac{\partial y}{\partial v_i} = \frac{df}{dx} \frac{\partial g}{\partial v_i}
$$

를 사용하고 있습니다.

$$\xi(p_m, \mathbf{v})$$의 미분은 탐색 결과의 말단 국면 평가함수의 미분으로 대체합니다.

말단 국면의 평가 함수는 KPP 관계의 선형 합이므로 미분이 가능합니다.

Bonanza에서는 수렴성을 높이기 위해 구속조건(보유 기물의 평가값 합을 일정하게 하는 조건)과 L1 정규화 항을 목적함수에 추가합니다.

목적 함수가 모든 국면 손실함수의 합이 되는 것에서 알 수 있듯이 일괄 학습을 하고 있습니다.

당시(2017년 즈음) 컴퓨터로 45833국 학습에 약 1개월이 걸린 것으로 추정됩니다.

### 마무리

알파고의 강화학습 Policy Network에서도 사용되는 것처럼 강화학습의 방법으로 Value Network에서 가치함수(=평가함수)를 학습할 수 있습니다.

학습한 Policy를 가지치기에 사용하여 학습한 평가함수를 탐색부의 최종 평가함수에 사용하는 것만으로 알파고 역시 컴퓨터 쇼기에서도 응용 가능할지도 모릅니다. 이러한 경우에는 Policy에서 사용한 함수가 KPP에서도 유효한지에 대해서는 테스트한 적이 없기 때문에 어떻게 될지는 알 수 없습니다.
