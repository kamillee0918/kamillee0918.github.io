---
layout: post
title: Bonanza 메서드 해설
date: 2023-09-08 12:22 +0800
last_modified_at: 2023-09-08 12:35 +0800
tags: [shogi, deep-learning]
math: true
toc: true
---

제가 오랫동안 관심을 가지고 있던 컴퓨터 쇼기(일본 장기)에 대해, 2017년에 개최된 제4회 전왕전(電王戦)을 다시 한번 살펴보려 합니다.

당시 명인 타이틀을 보유하고 있던 사토 아마히코(佐藤天彦)는 컴퓨터 쇼기 소프트웨어인 [Ponanza(ポナンザ)](https://ja.wikipedia.org/wiki/Ponanza)에게 패배를 당하였습니다. 이 때 사용된 Ponanza의 버전은 미니맥스 알고리즘의 개선판으로서, 평가 함수를 기계학습하여 최적화하는 방식으로 작동하였습니다.

미니맥스 알고리즘이 병렬화 및 고속화되는 것은 체스 프로그램 등에서 이미 오래 전부터 이루어진 개선사항입니다.

쇼기 프로그램에서도 기본은 크게 다르지 않으며 컴퓨터 쇼기에서의 기술적인 발전은 그다지 크지 않았다고 생각합니다.

평가 함수에 관해서는 2006년 [Bonanza(ボナンザ)](https://ja.wikipedia.org/wiki/Bonanza) 등장 이전에는 개발자가 로직을 짜서 파라미터 등을 조정하는 것이 보통이었지만 Bonanza는 프로 기사들의 기보로부터 KPP(왕(King) 기물을 중심으로 특정 말의 기물들이 어떠한 형태(囲い)를 맺고 있는지)를 통해 이러한 특징값의 선형 합을 기계학습 하면서 컴퓨터 쇼기의 획기적인 발전을 이루었습니다.

이 소프트의 등장 이후 대부분의 쇼기 프로그램은 Bonanza에서 비롯되었다고 할 수 있습니다.

Ponanza는 다른 쇼기 프로그램과의 차별점은 평가 함수 학습에 프로 기사들의 기보를 사용하지 않고, 자기 대국 결과를 이용해 강화학습을 하는 것으로 추측됩니다.

Bonanza가 도입한 기계학습 기반의 평가 함수는 현재 쇼기 프로그램에서 중요한 요소로 인식되고 있습니다.

이번 포스팅에서는 Bonanza가 사용하는 기계학습 방법, 일명 'Bonanza 메서드'에 대해 설명하려 합니다.

### Bonanza 메서드

학습할 국면 집합을 $$\mathbf{P}$$ 로, 특징 벡터를 $$\mathbf{v}$$로 정의하며, 각각의 국면 p에서 가능한 모든 수를 $$M_p$$로 설정합니다. 기보에서 선택된 자식 국면은 $$p_1$$으로 나타내고, m번째 합법적인 수에 의해 생성된 자식 국면은 $$p_m$$으로 표시합니다. 이렇게 설정하면 목적 함수 $$J(\mathbf{P},\mathbf{v})$$는 다음과 같이 설계됩니다.

$$
J(\mathbf{P},\mathbf{v})=\sum_{p∈\mathbf{P}}\sum_{m=2}^{Mp} T_p[\xi(p_m, \mathbf{v})−\xi(p_1,\mathbf{v})]
$$

위의 수식에서 $$\xi(p_m, \mathbf{v})$$는 미니맥스 탐색치, $$T_p(x)$$는 손실 함수이며, 국면 p의 차례가 Max 플레이어일 때 $$T_p(x)=T(+x)$$이고, Min 플레이어의 경우 $$T_p(x)=T(-x)$$가 됩니다.

$$T(x)$$는 단조로운 증가 함수인 경우가 좋으며 Bonanza에서는 시그모이드 함수 $$1/(1+e^−0.0273x)$$가 사용되고 있습니다.

경사 하강법을 사용해 목표 함수를 최소화하도록 특징 벡터 $$\mathbf{v}$$를 학습합니다.

### 해설

프로 기보에서 선택된 차례에 대해 정확한 평가치를 알아낼 방법이 없기 때문에, 직접적으로 평가 함수를 학습하는 것은 불가능합니다.

그래서 현재 사용되는 평가 함수와 프로 기보에서 선택된 국면 그리고 그 외 국면들 간 차이점들을 총합하여 손실 함수 $$T(x)$$라고 정리하였습니다.

프로의 기보와 다른 수를 선택했을 경우, 그 평가치를 낮게 추정할수록 손실 함수의 값은 커집니다.

모든 국면에 대해 계산된 손실 함수의 합이 목적 함수가 됩니다.

손실 함수가 미분 가능한 형태라면, 경사 하강법을 이용하여 특징 벡터 $$\mathbf{v}$$를 최적화할 수 있습니다.

경사 하강법은 목적함수를 $$v_i$$ 에 대해 편미분을 수행합니다.

$$
\frac{\partial J(\mathbf{P},\mathbf{v})}{\partial v_i}=\sum_{p∈\mathbf{P}}\sum_{m=2}^{M_p} \frac{dT_p(x)}{dx} [\frac{\partial \xi(pm,\mathbf{v})}{\partial v_i} − \frac{\partial \xi(p1,\mathbf{v})}{\partial v_i}]
$$

각 파라미터 $$v_i$$에 대해 편미분 값에 학습률을 곱하여 파라미터를 업데이트합니다.

위 식에서 $$T_p(x)$$의 $$v_i$$에 대한 편미분은 합성함수의 미분 법칙

$$
y=f(x), x=g(\mathbf{v})일 때, \frac{\partial y}{\partial v_i} = \frac{df}{dx} \frac{\partial g}{\partial v_i}
$$

를 사용하고 있습니다.

$$\xi(p_m, \mathbf{v})$$의 미분은 탐색 결과의 말단 국면 평가함수의 미분으로 대체합니다.

말단 국면의 평가 함수는 KPP 관계를 선형 결합으로 나타내기 때문에, 이는 미분이 가능합니다.

Bonanza에서는 수렴성을 높이기 위해 구속조건(보유 기물의 평가치 합을 일정하게 하는 조건)과 L1 정규화 항을 목적 함수에 추가합니다.

목적 함수가 모든 국면 손실 함수의 합이 되는 것에서 알 수 있듯이 일괄 학습하고 있습니다.

당시(약 2017년)에는 컴퓨터로 45,833 대국을 학습하는데 대략 한 달 정도 소요되었다고 추정됩니다.

### 마무리

알파고의 Policy Network에서 사용된 것과 같은 강화학습 방식으로, Value Network에서 가치 함수(또는 평가 함수)를 학습하는 것이 가능합니다.

학습된 Policy를 가지치기에 활용하고, 그 결과로 학습된 평가 함수를 탐색 과정의 최종 평가 함수로 이용한다면, 알파고 방식이 컴퓨터 쇼기에도 응용될 수 있을지 모르겠습니다.

그렇게 된다면, Policy에서 사용된 함수가 KPP 상황에도 유효할 지 여부는 아직 테스트해 보지 않아서 확실하지 않습니다.
